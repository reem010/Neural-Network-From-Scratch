# Neural-Network-From-Scratch
<div>
  <p>I have successfully completed the following steps to implement a dynamic Neural Network from scratch for the MNIST dataset:</p>
  <ol>
    <li>Loaded the MNIST dataset.</li>
    <li>Standardized the dataset to ensure consistent scaling.</li>
    <li>Divided the data into training and test sets for model evaluation.</li>
    <li>Applied the one-hot vector encoding technique to the labels. This involved representing each label as a vector of size 10, where the value is 1 in the correct class and 0 in the rest.</li>
    <li>Created a dynamic Neural Network from scratch, taking ownership of the implementation. Here's what I accomplished:
      <ul>
        <li>Initialized the weights of the layers with random values to introduce variability.</li>
        <li>Utilized equations to compute the output for all forward passes, enabling information flow through the network.</li>
        <li>Employed the sigmoid function as the activation function for both the final output layer and hidden layers, ensuring non-linearity.</li>
        <li>Chose Mean Squared Error (MSE) as the error function to measure the difference between the one-hot vector and the Neural Network's prediction vector.</li>
        <li>Implemented backpropagation to update the weights, optimizing the network's performance through gradient descent.</li>
      </ul>
    </li>
  </ol>
  <p>Throughout this process, I gained valuable insights into the inner workings of Neural Networks and their application to image classification tasks.</p>
</div>

